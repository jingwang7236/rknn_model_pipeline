## 2. 算法库使用说明

测试用例在rk3588_test_pipeline目录下，包含以下算法：

### 2.1. 目标检测

包含行人检测、人脸检测、头部检测、手机检测;

#### 2.1.1. 行人检测

模型结构：[yolov10](https://github.com/THU-MIG/yolov10)

调用方法：

```
include "outer_model/model_func.hpp"
include "outer_model/model_params.hpp"

det_model_input input_data;  // 输入数据格式
input_data.data = data;   // rgb24格式
input_data.width = width;
input_data.height = height;
input_data.channel = channel;
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
const char* model_path = "model/yolov10s.rknn";
ret = init_model(model_path, &rknn_app_ctx);  // 初始化模型
object_detect_result_list result = inference_person_det_model(&rknn_app_ctx, input_data, true); //模型推理
ret = release_model(&rknn_app_ctx); // 释放模型资源
```

#### 2.1.2. 人脸检测

模型结构：[RetinaFace](https://github.com/airockchip/rknn_model_zoo/tree/main/examples/RetinaFace)

链接：

调用方法：

```
include "outer_model/model_func.hpp"
include "outer_model/model_params.hpp"

const char* model_path = "model/RetinaFace.rknn";
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
ret = init_model(model_path, &rknn_app_ctx);  // 初始化
retinaface_result result = inference_face_det_model(&rknn_app_ctx, input_data, true); //推理
ret = release_model(&rknn_app_ctx);  //释放
```

#### 2.1.3. 头肩检测

模型结构：Retinanet, RepVgg

模型输出：头部的box

调用方法：

```
include "outer_model/model_func.hpp"
include "outer_model/model_params.hpp"

const char* model_path = "model/HeaderDet.rknn";
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
ret = init_model(model_path, &rknn_app_ctx);  // 初始化
ssd_det_result result = inference_header_det_model(&rknn_app_ctx, input_data, true); //推理
ret = release_model(&rknn_app_ctx);  //释放
```

#### 2.1.4. 手机检测

模型结构：Retinanet, RepVgg

模型输出：手机的box

调用方法：

```
include "outer_model/model_func.hpp"
include "outer_model/model_params.hpp"

const char* model_path = "model/PhoneDet.rknn";
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
ret = init_model(model_path, &rknn_app_ctx);  // 初始化
ssd_det_result result = inference_phone_det_model(&rknn_app_ctx, input_data, true); //推理
ret = release_model(&rknn_app_ctx);  //释放
```

#### 2.1.5. 手部检测

模型说明：

> 检测并获取图片中的手部区域框。

图片输入：`监控原图`

模型结构：`YOLOv8-det`

模型名称：`det_hand_s_yy_mm_dd.rknn`

类别标签：`0: hand（手）`

模型输出：`object_detect_result_list`

```c
// 目标框bbox的结构体
typedef struct {
    int left;    // 左上角点横坐标
    int top;    // 左上角点纵坐标
    int right;    // 右下角点横坐标
    int bottom;    // 右下角点纵坐标
} image_rect_t;

// 目标的结构体
typedef struct {
    image_rect_t box;
    float prop;    // 检测框的置信度
    int cls_id;    // 检测框的类别
} object_detect_result;

// 检测结果的结构体
typedef struct {
    int id;    // 模型类型，暂时没用到
    int count;    // 检测结果数目
    object_detect_result results[OBJ_NUMB_MAX_SIZE];    // 检测结果列表
} object_detect_result_list;
```

调用方法：

```c
object_detect_result_list inference_det_hand_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data,
    bool det_by_square = true,
    /* det_by_square 设置为true：
       启用方形裁剪，用一张图的较小边裁剪出n个正方形区域，对每个区域进行推理，
       n = int(max((w + h - 1) / h, (h + w - 1) / w)) + 1,
       注意：该设置仅对较小目标启用（如监控下的手），模型推理耗时是原图推理的n倍。
       推理结果极小概率会存在重复检测框，但不影响。*/
    bool enable_logger = true 
);
```

#### 2.1.6 款箱检测

模型说明：

> 检测并获取图片中的款箱区域框。

图片输入：`监控原图`

模型结构：`YOLOv8-det`

模型名称：`det_kx_s_yy_mm_dd.rknn`

模型输出：`object_detect_result_list`

类别标签：`0: kx（关闭的款箱）, 1: kx_dk（打开的款箱）`

调用方法：

```c
object_detect_result_list inference_det_kx_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger);
```

#### 2.1.7 枪支检测

模型说明：

> 检测并获取图片中的霰弹枪区域框。

图片输入：`原图`

模型输出：`object_detect_result_list`

类别标签：`0: gun`

调用接口：

```c
object_detect_result_list inference_det_gun_model(rknn_app_context_t* app_ctx, det_model_input input_data, model_inference_params params_, bool det_by_square, bool enable_logger);

```

```c
调用示例：
/* 推理参数 width height nms_ths box_ths*/
/* 注意!!! 这里传入的是模型的推理尺寸，而非输入图片的尺寸 */
model_inference_params params_det_gun = { 640,640,0.6f,0.25f };
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
const char* model_path = "model/gun_i8.rknn";
// const char* label_txt_path = "model/classes_gun.txt";
ret = init_model(model_path, &rknn_app_ctx);
if (ret != 0)
{
    printf("init_yolov8_model fail! ret=%d model_path=%s\n", ret, model_path);
    return -1;
}

rknn_app_ctx.is_quant = true;

//print_rknn_app_context(rknn_app_ctx);

object_detect_result_list result = inference_det_gun_model(&rknn_app_ctx, input_data, params_det_gun, false, true); //推理
ret = release_model(&rknn_app_ctx);
if (ret != 0)
{
    printf("release_yolov8_model fail! ret=%d\n", ret);
}

```

#### 2.1.8 甩棍检测（旋转框）

模型说明：

> 检测并获取图片中的手持甩棍旋转框。
> 与矩形框不同的是，旋转框返回结果是 xywhr  即，中心点坐标(xy)，框的宽高(wh)，旋转角度
图片输入：`原图`

模型输出：`object_detect_obb_result_list`

类别标签：`0: stick`

调用接口：

```c
object_detect_obb_result_list inference_obb_stick_model(rknn_app_context_t* app_ctx, det_model_input input_data, model_inference_params params_, bool det_by_square, bool enable_logger);

```

```c
调用示例：
/* 推理参数 width height nms_ths box_ths*/
/* 注意!!! 这里传入的是模型的推理尺寸，而非输入图片的尺寸 */
model_inference_params params_det_gun = { 1024,1024,0.6f,0.25f };
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
const char* model_path = "../model/obb.rknn";
ret = init_model(model_path, &rknn_app_ctx);
if (ret != 0)
{
    printf("init_yolov8_model fail! ret=%d model_path=%s\n", ret, model_path);
    return -1;
}

rknn_app_ctx.is_quant = false;

//print_rknn_app_context(rknn_app_ctx);

object_detect_obb_result_list result = inference_obb_stick_model(&rknn_app_ctx, input_data, params_det_gun, false, true); //推理
ret = release_model(&rknn_app_ctx);
if (ret != 0)
{
    printf("release_yolov8_model fail! ret=%d\n", ret);
}

```

#### 2.1.9 门状态检测

模型说明：

> 检测并获取图片中的门区域框，并给出开关状态类别。

图片输入：`原图`

模型输出：`object_detect_result_list`

类别标签：`0: closed 1:open`

调用接口：

```c
object_detect_result_list inference_det_stat_door_model(rknn_app_context_t* app_ctx, det_model_input input_data, model_inference_params params_, bool det_by_square, bool enable_logger);

```

```c
调用示例：
/* 推理参数 width height nms_ths box_ths*/
/* 注意!!! 这里传入的是模型的推理尺寸，而非输入图片的尺寸 */
model_inference_params params_det_stat_door = { 640,640,0.6f,0.25f };
rknn_app_context_t rknn_app_ctx;
memset(&rknn_app_ctx, 0, sizeof(rknn_app_context_t));
const char* model_path = "../model/door.rknn";
//const char* label_txt_path = "model/classes_door.txt";
ret = init_model(model_path, &rknn_app_ctx);
if (ret != 0)
{
    printf("init_yolov8_model fail! ret=%d model_path=%s\n", ret, model_path);
    return -1;
}

rknn_app_ctx.is_quant = false;

print_rknn_app_context(rknn_app_ctx);

object_detect_result_list result = inference_det_stat_door_model(&rknn_app_ctx, input_data, params_det_stat_door, false, true); //推理
ret = release_model(&rknn_app_ctx);
if (ret != 0)
{
    printf("release_yolov8_model fail! ret=%d\n", ret);
}

```


### 2.2. 分类

包含人脸属性；

#### 2.2.1. 人脸属性/可疑人员

模型组合：头部检测 + 人脸属性；输入为场景大图，先经过头部检测模型，再将检测到的小图经过人脸属性模型推理，得到人脸属性结果；

输出结果: 人脸属性结果；

模型结构：RepVgg

模型输出：[int, int, int], 依次表示人脸是否被遮挡的三个属性;

（1）是否带帽子或头盔（0-什么都不带、1-带帽子、2-戴头盔）;

（2）是否带墨镜（0-不带墨镜、1-带墨镜）;

（3）是否戴口罩（0-不带口罩、1-戴口罩）；

调用方法：

```
include "outer_model/model_func.hpp"
include "outer_model/model_params.hpp"

// 检测初始化
const char* det_model_path = "model/HeaderDet.rknn";
rknn_app_context_t det_rknn_app_ctx;
memset(&det_rknn_app_ctx, 0, sizeof(rknn_app_context_t));
ret = init_model(det_model_path, &det_rknn_app_ctx);  
// 分类初始化
rknn_app_context_t cls_rknn_app_ctx;
memset(&cls_rknn_app_ctx, 0, sizeof(rknn_app_context_t));
const char* cls_model_path = "model/FaceAttr.rknn";
ret = init_model(cls_model_path, &cls_rknn_app_ctx);
ssd_det_result det_result = inference_header_det_model(&det_rknn_app_ctx, input_data, true); //头肩检测模型推理
det_result.count = det_result.count;
for (int i = 0; i < det_result.count; ++i) {
    box_rect header_box;  // header的box
    header_box.left = std::max(det_result.object[i].box.left, 0);
    header_box.top = std::max(det_result.object[i].box.top, 0);
    header_box.right = std::min(det_result.object[i].box.right, width);
    header_box.bottom = std::min(det_result.object[i].box.bottom, height);
    // 人脸属性模型
    cls_result cls_result = inference_face_attr_model(&cls_rknn_app_ctx, input_data, header_box, true);
}
ret = release_model(&det_rknn_app_ctx);  //释放
ret = release_model(&cls_rknn_app_ctx);
```

#### 2.2.2. OCR识别

包含OCR检测和识别；

```
    const char* det_model_path = "model/ppocrv4_det.rknn";
    const char* rec_model_path = "model/ppocrv4_rec.rknn";
    ppocr_system_app_context rknn_app_ctx;  
    memset(&rknn_app_ctx, 0, sizeof(ppocr_system_app_context));// 初始化
    ret = init_model(det_model_path, &rknn_app_ctx.det_context);
    ret = init_model(rec_model_path, &rknn_app_ctx.rec_context);
    ppocr_text_recog_array_result_t results = inference_ppocr_det_rec_model(&rknn_app_ctx, input_data, true);
    ret = release_model(&rknn_app_ctx.det_context);// 释放
    ret = release_model(&rknn_app_ctx.rec_context);
```

#### 2.2.3 交行五类人员识别

图片输入：`行人检测框内的图片patch`

模型结构：`resnet18`

模型名称：`rec_ren_mmdd_resnet18.rknn`

模型输出：`resnet_result`

```c
// ResNet
typedef struct {
    int cls;    // 类别
    float score;    // 置信度
} resnet_result;
```

类别标签：`0:anbao（保安）, 1:baojie(保洁), 2:worker（行员）, 3:yayun (押运), 4:ren（无关人员）`

调用方法：

```c
resnet_result inference_rec_person_resnet18_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger=false);
```

#### 2.2.4 交行五类手部动作识别

模型说明：

> 识别交行场景的五类手势，主要用到签名这一类。

图片输入：`手部检测框放大2/3的图片patch`

模型结构：`resnet18`

模型名称：`rec_hand_mmdd_resnet18.rknn`

模型输出：`resnet_result`

类别标签：`0:gp（拿工牌）, 1:qm(签名), 2:wsj（玩手机）, 3:gp_sz (验工牌时指墙面，该动作必是在验工牌), 4:other（无关手势）`

调用方法：

```c
resnet_result inference_rec_hand_resnet18_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger);
```

#### 2.2.5 交行款箱方向识别

模型说明：

> 识别款箱的朝向。

图片输入：`款箱检测框内的图片patch`

模型结构：`resnet18`

模型名称：`rec_kx_orient_resnet18.rknn`

模型输出：`resnet_result`

类别标签：`0:kx_hp（水平款箱）, 1:kx_sz(竖直款箱，无效图片)`

调用方法：

```c
resnet_result inference_rec_kx_orient_resnet18_model(
    rknn_app_context_t *app_ctx,
    det_model_input input_data, 
    bool enable_logger)
```

#### 2.2.6 门区域开关状态分类

模型说明：

> 识别划定区域内门的开关状态。

图片输入：`前端输入的图片patch`

模型结构：`resnet18`

模型输出：`resnet_result`

类别标签：`0:closed, 1:open ,2:other(该区域不是门)`

调用接口：

```c
resnet_result inference_rec_stat_door_resnet18_model(rknn_app_context_t* app_ctx, det_model_input input_data, bool enable_logger);

```

调用示例：

```c
const char* model_path = "../model/cls_door_i8.rknn";
rknn_app_context_t rec_rknn_app_ctx;
memset(&rec_rknn_app_ctx, 0, sizeof(rknn_app_context_t));
ret = init_model(model_path, &rec_rknn_app_ctx);

if (ret != 0)
{
    printf("init_rec_ren_model fail! ret=%d model_path=%s\n", ret, model_path);
    return -1;
}

resnet_result rec_result = inference_rec_stat_door_resnet18_model(&rec_rknn_app_ctx, input_data, false);
std::cout << "Class index: " << rec_result.cls << ", Score: " << rec_result.score << std::endl;
ret = release_model(&rec_rknn_app_ctx);

```


### 2.3 关键点检测

#### 2.3.1 人体关键点

模型说明：

> 获取17个人体关键点。

图片输入：`监控原图`

模型结构：`YOLOv8-pose`

模型名称：`yolov8s-pose.rknn`

类别标签：

> 检测框标签（1个类别）：
>     0: person（人）
> 
> 关键点标签（17个关键点）：
> 
>     coco_person_keypoint_class_name = {  
>  0: 'nose',  
>  1: 'left_eye',  
>  2: 'right_eye',  
>  3: 'left_ear',  
>  4: 'right_ear',  
>  5: 'left_shoulder',  
>  6: 'right_shoulder',  
>  7: 'left_elbow',     // 左手肘
>  8: 'right_elbow',     // 右手肘
>  9: 'left_wrist',     // 左手腕
>  10: 'right_wrist',     // 有手腕
>  11: 'left_hip',  
>  12: 'right_hip',  
>  13: 'left_knee',  
>  14: 'right_knee',  
>  15: 'left_ankle',  
>  16: 'right_ankle',
> 
> }

模型输出：`object_detect_pose_result_list`

```c
// 关键点结构体
typedef struct {
    image_rect_t box;    // 检测框
    float keypoints[17][3];    //keypoints x,y,conf（后续修改点数）
    float prop;    // 检测框置信度
    int cls_id;    // 检测框的类别ID，一般只有一个类别，都为0
} object_detect_pose_result;

// 关键点结果列表结构体
typedef struct {
    int id;
    int count;    // 结果个数
    object_detect_pose_result results[OBJ_NUMB_MAX_SIZE];    // 结果列表
} object_detect_pose_result_list;
```

调用方法：

```c
object_detect_pose_result_list inference_pose_ren_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger);
```

关键点可视化：

![人体姿态关键点示例](https://i-blog.csdnimg.cn/blog_migrate/59bd83f3197cdc00555749bef137de07.png)


#### 2.3.2 水平款箱关键点

模型说明：

> 获取水平放置款箱的6个关键点。

图片输入：`款箱检测框放大2/3的图片patch`

模型结构：`YOLOv8-pose`

模型名称：`pose_kx_hp_s_yy_mm_dd.rknn`

类别标签：

> 检测框标签（1个类别）： 
>     0: kx_hp（横平放置的款箱）
> 
> 关键点标签（6个关键点）：  
> 0: 'zs',     // 款箱左上角点
> 1: 'ys',     // 款箱右上角点
> 2: 'yx',     // 款箱右下角点
> 3: 'zx',     // 款箱左下角点
> 4: 'zsk',    // 款箱左锁扣点
> 5: 'ysk',    // 款箱右锁扣点

模型输出：`object_detect_pose_result_list`

> 先使用人体关键点的数据结构，后续会进行修改。

调用方法：

```c
object_detect_pose_result_list inference_pose_kx_hp_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger);
```

关键点可视化：

![水平款箱关键点示例](https://github.com/jingwang7236/rknn_model_pipeline/blob/cj/doc/images/kx_hp_kpt.png)


#### 2.3.3 竖直款箱关键点

模型说明：

> 获取竖直放置款箱的2个关键点。

图片输入：`款箱检测框放大2/3的图片patch`

模型结构：`YOLOv8-pose`

模型名称：`pose_kx_sz_s_yy_mm_dd.rknn`

类别标签：

> 检测框标签（1个类别）：
>     0: kx_sz（竖直放置的款箱）
> 
> 关键点标签（2个关键点）：  
> 0: 'zsk',    // 款箱左锁扣点
> 1: 'ysk',    // 款箱右锁扣点

模型输出：`object_detect_pose_result_list`

> 先使用人体关键点的数据结构，后续会进行修改。

调用方法：

```c
object_detect_pose_result_list inference_pose_kx_sz_model(
    rknn_app_context_t *app_ctx, 
    det_model_input input_data, 
    bool enable_logger);
```

关键点可视化：

![竖直款箱关键点示例](https://github.com/jingwang7236/rknn_model_pipeline/blob/cj/doc/images/kx_sz_kpt.png)




### 2.4. 测试模型指标

#### 2.4.1. 分类模型指标

```
  if (std::string(model_name) == "test_face_attr"){ 
      // 创建分类模型管理器并添加模型
      ClsModelManager clsmodelManager;
      clsmodelManager.addModel("FaceAttr", "model/FaceAttr.rknn", inference_face_attr_model); // 模型名、模型路径、推理函数
      ret = ClsModelAccuracyCalculator(clsmodelManager, "FaceAttr", image_path);
      return ret;
  }
```

#### 2.4.2.检测模型指标
```
  if (std::string(model_name) == "test_person_det"){
      // 定义label_name和label_id的映射关系，传入函数
      std::map<std::string, int> label_name_map = {
          {"ren", 0},  // labelme标注工具：label_name和模型输出的label_id对应
      };
      float CONF_THRESHOLD = 0.5; // 计算某个阈值的PR
      float NMS_THRESHOLD = 0.45; // 计算MAP
      DetModelManager modelManager; // 创建检测模型管理器并添加模型
      modelManager.addModel("PersonDet", "model/yolov10s.rknn", inference_person_det_model);
      ret = DetModelMapCalculator(modelManager, "PersonDet", image_path, label_name_map, CONF_THRESHOLD, NMS_THRESHOLD);
      return ret;
  }
```



